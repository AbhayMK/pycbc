#!/usr/bin/env python

__prog__ = 'pycbc_overlaps'
__author__ = 'Collin Capano <collin.capano@ligo.org>'
__description__ = 'Calculates effectualness for a set of signals given a template bank. Values are stored for the best match.'

import sqlite3
import numpy
import os, sys, shutil, getpass
import time
import random
import operator
from optparse import OptionParser

import lal
import lalsimulation as lalsim

from glue.ligolw import utils
from glue.ligolw import table
from glue.ligolw import lsctables
from glue.ligolw import dbtables
from glue.ligolw.utils import process

from pylal import ligolw_sqlutils as sqlutils

import pycbc
from pycbc import filter
from pycbc import types as pytypes

from pycbc.overlaps import waveform_utils
from pycbc.overlaps import overlap_utils

parser = OptionParser(description = __description__)
parser.add_option('-o', '--output-dir', default = '.', help = 'Directory to save overlap output data.')
parser.add_option("-t", "--tmp-space", action = "store", type = "string", default = None, metavar = "PATH", help = "Location of local disk on which to do work. This is used to enhance performance in a networked environment.")
parser.add_option("-p", "--psd-model", help = "What PSD model to use for overlaps. Options are %s." %(', '.join(overlap_utils.get_psd_models())))
parser.add_option("-A", "--asd-file", help = "Load the PSD from a dat file containing an ASD. If both this and psd-model specified, this will take precedence.")
parser.add_option('-f', '--waveform-f-min', type = 'float', help = "Frequency at which to start the waveform generation (in Hz).")
parser.add_option('-F', '--overlap-f-min', type = 'float', help = "Frequency at which to start the overlap (in Hz). Note: This must be larger than waveform-f-min.")
parser.add_option('-r', '--sample-rate', type = 'int', help = "Required. Sample rate to use (in Hz). Must be a power of 2. If vary-n is turned on and the sample rate used to generate the waveforms and compute overlaps is less than this, the effectulaness time series will be upsampled to this sample rate. If the sample rate used in the overlap calculation is greater than this, that sample rate will be used.")
parser.add_option('-l', '--segment-length', type = 'int', help = "Segment length to use (in s). Must be a power of 2. Required if vary-n not used.")
parser.add_option('-n', '--vary-n', action='store_true', default=False, help='If turned on, the sample rate and segment length used will be set to the smallest possible value between a given template and injection. This speeds up overlap calculations, but may reduce the accuracy of the effectualness measurement.')
parser.add_option('-P', '--precision', metavar="[double|single]", default='double', help="What precision to use. Options are 'double' or 'single'. Default is double.")
parser.add_option('-D', '--dyn-range-exp', type=int, default=int(numpy.log2(pycbc.DYN_RANGE_FAC)), help="What power to use for the dynamic range factor, which = 2**dyn_range_exp. This is necessary if using single precision. Default is %i." %(int(numpy.log2(pycbc.DYN_RANGE_FAC))))
parser.add_option('-i', '--ifo', help = "What ifo to inject into. If none specified, no response function will be applied to the injections.")
parser.add_option('-U', '--user-tag', help = "User tag to add to all Overlap files.")
parser.add_option('-M', '--use-injection-cache', metavar = '"memory" or "disk"', help = "Speed up calculations by using an injection cache. Options are 'memory' or 'disk'. If 'memory', all injections will be stored in memory. If 'disk', injections will be stored to a temporary hdf5 archive. If a temp-space is specified, this temporary archive will be placed there; otherwise, it will be created in the current working directory.")
parser.add_option('-a', '--approximant', help = "Approximant to use for the templates.")
parser.add_option("", "--amp-order", type=int, default=0, help="Amplitude order to use for templates. Default is 0.")
parser.add_option("", "--phase-order", type=int, default=7, help="Phase order to use for templates. Default is 7 (3.5PN).")
parser.add_option("", "--spin-order", type=int, default=None, help="Spin order to use for templates.")
parser.add_option("", "--taper", help = "For TD approximants, set whether or not to taper the templates at the start and/or end. Options are: start, end, start_end. If not specified, no tapering will be done.")
parser.add_option("", "--apply-weights", metavar="WEIGHT1[,WEIGHT2,...]", default="uniform", help="Apply a weight to a template when maximizing the overlap of an injection over the bank. Options are %s. Default is uniform. To specify more than one weight, separate the weights by commas. A separate maximization will be performed for each weight specified. The name of the weight associated with each maximization is stored in the description column of the coinc_definer table." %(', '.join(overlap_utils.weight_functions.keys())))
parser.add_option('', '--simple-window', metavar='PARAM:LOWER[:UPPER]', help="Only filter templates that lie within the given fractional range of the given parameter of an injection. LOWER must be < 1. If an UPPER is not specified, the LOWER fractional range will be used. For example, if set to mchirp:0.2, only templates that have an mchirp within +/-0.2 of an injection's mchirp will be filtered with that injection. If set to mchirp:0.2:0.5, templates with an mchirp [- 0.2, +0.5] * the injection's mchirp will be filtered.")
parser.add_option('', '--match-window-db', help = 'Use the match windows specified in the given database to reduce the number of overlaps computed. This can provide a more fine-tuned match window than that provided by use-simple-window. This cannot be used at the same time as use-simple-window.')
parser.add_option('', '--save-all-overlaps', action = 'store_true', default = False, help = 'Do not delete the all_overlaps table from the output database before exiting. This table contains the fitting factors for all the injections and templates that were filtered.')
parser.add_option('-R', '--replace', action = 'store_true', default = False, help = 'If the output database already exists, overwrite it. Default action is not to overwrite, in which case only injections that do not have overlap values calculated in the database will be analyzed.')
parser.add_option('-T', '--checkpoint-dt', type = 'float', default = numpy.inf, help = 'Number of minutes between checkpoints. Default is infinity, meaning no checkpointing.')
parser.add_option('-v', '--verbose', action = 'store_true', help = 'Be verbose.')

opts, filenames = parser.parse_args()

# check options
ifo = opts.ifo
if opts.ifo is not None:
    ifo = ifo.upper()
    if len(ifo) != 2:
        raise ValueError("--ifo must be of format [site][number], e.g., 'H1'")
wfmin = opts.waveform_f_min
ofmin = opts.overlap_f_min
if ofmin < wfmin:
    raise ValueError("--waveform-f-min must be less than overlap-f-min")

if opts.sample_rate is None:
    raise ValueError("must specify a sample-rate")
target_sample_rate = opts.sample_rate
if numpy.log2(target_sample_rate) % 1 != 0.:
    raise ValueError("--sample-rate must be a power of 2")

if not opts.vary_n:
    sample_rate = target_sample_rate
    if sample_rate is None or numpy.log2(sample_rate) % 1 != 0.:
        raise ValueError("--sample-rate must be a power of 2")
    if opts.segment_length is None:
        raise ValueError("if not --vary-n, must specify a segment-length")
    seg_length = opts.segment_length
    if seg_length is None or numpy.log2(seg_length) % 1 != 0.:
        raise ValueError("--segment-length must be a power of 2")
    N = sample_rate * seg_length
    df = 1./seg_length

if opts.asd_file is None and opts.psd_model is None:
    raise ValueError("must specify --asd-file or --psd-model")
if opts.asd_file is not None and not os.path.exists(opts.asd_file):
    raise ValueError("asd-file %s not found" % opts.asd_file)
if (opts.use_injection_cache is not None and not 
        (opts.use_injection_cache == 'disk' or
        opts.use_injection_cache == 'memory')):
    raise ValueError('if using injection cache, --use-injection-cache must ' +\
        'be set to "disk" or "memory"')
taper = waveform_utils.get_taper_string(opts.taper)

dyn_range_fac = 2.**opts.dyn_range_exp
if opts.precision.lower() == 'single':
    real_type = pytypes.float32
    cmplx_type = pytypes.complex64
elif opts.precision.lower() == 'double':
    real_type = pytypes.float64
    cmplx_type = pytypes.complex128
else:
    raise ValueError, "unrecognized precision %s" %(opts.precision)

# parse weights
weight_functions = map(str.lower, map(str.strip,
    opts.apply_weights.split(',')))
for weight_func in weight_functions:
    if weight_func not in overlap_utils.weight_functions:
        raise ValueError("unrecognized weight %s; options are %s" %(
            weight_func, ', '.join(overlap_utils.weight_functions.keys())))

#
#   Check for parameter windows
#
if opts.match_window_db is not None and opts.simple_window is not None:
    raise ValueError("Cannot use a match-window-db and a simple-window at " +
        "the same time. Please choose one, or neither.")

if opts.simple_window is not None:
    win_settings = opts.simple_window.split(':')
    try:
        param, min_jitter, max_jitter = win_settings
    except ValueError:
        # max_jitter not specified
        try:
            param, min_jitter = win_settings
            max_jitter = min_jitter
        except ValueError:
            # option not configured properly
            raise ValueError("Simple-window arguments not formatted " +
                "correctly; see help message.")
    min_jitter = -1.*float(min_jitter)
    if abs(min_jitter) > 1.:
        raise ValueError("Lower bound of simple-window cannot be > 1; " +
            "see help message.")
    max_jitter = float(max_jitter)
    pwin = overlap_utils.ParamWindow(-numpy.inf, numpy.inf)
    pwin.set_jitter(min_jitter, max_jitter)
elif opts.match_window_db is not None:
    param_windows = {}
    connection = sqlite3.connect(opts.match_window_db)
    sqlquery = '''
        SELECT DISTINCT
            inj_apprx
        FROM
            match_windows
        WHERE
            tmplt_apprx == ?
        '''
    apprxs = [apprx[0] for apprx in connection.cursor().execute(sqlquery,
        (opts.approximant,))]
    for apprx in apprxs:
        pwList = overlap_utils.ParamWindowList()
        pwList.load_from_database(connection, 'mchirp', apprx,
            tmplt_apprx=opts.approximant)
        param_windows[apprx] = pwList
else:
    # we'll just create a dummy function that always evaluates to True
    class DummyWindow:
        def in_recovered(self, a, b):
            return True
    pwin = DummyWindow()

# initialize the workspace
workSpace = overlap_utils.WorkSpace()

# set the start time
last_time = time.time()
last_timestamp = 0
last_keylen = 0
last_subkeylen = 0

# set a temp path for backup files
tmp_path = opts.tmp_space is not None and opts.tmp_space or '.'

for fnum, infile in enumerate(filenames):

    if opts.verbose:
        print >> sys.stdout, "Analyzing file %s" % infile
    if not os.path.exists(infile):
        raise ValueError, "the input file %s could not be found" % infile
    # get output name
    outfile = overlap_utils.get_outfilename(opts.output_dir, opts.ifo,
        opts.user_tag, fnum)
    if outfile == infile:
        raise ValueError("output file (%s) is the same as the input file " +\
            "(%s)" %(outfile, infile))

    if opts.tmp_space:
        if os.path.exists(outfile) and not opts.replace:
            working_filename = dbtables.get_connection_filename(outfile,
                tmp_path=opts.tmp_space, verbose=opts.verbose)
        else:
            working_filename = dbtables.get_connection_filename(infile,
                tmp_path=opts.tmp_space, verbose=opts.verbose)
        connection = sqlite3.connect(working_filename)
        dbtables.set_temp_store_directory(connection, opts.tmp_space,
            verbose=opts.verbose)
    else:
        if not os.path.exists(outfile) or opts.replace:
            connection = overlap_utils.get_outfile_connection(infile, outfile,
                replace=opts.replace, verbose=opts.verbose)
        else:
            connection = sqlite3.connect(outfile)
        working_filename = None
    # FIXME: dbtables apparently has a problem with overlap_results table
    try:
        xmldoc = dbtables.get_xml(connection)
        this_process = process.register_to_xmldoc(xmldoc, __prog__,
            opts.__dict__).process_id
    except:
        print >> sys.stderr, "Warning: not writing process info to database"
        this_process = sqlutils.get_next_id(connection, 'process',
            'process_id')

    # create the results table in the output if it doesn't exist
    overlap_utils.create_results_table(connection)
    overlap_utils.create_cem_table(connection)
    overlap_utils.create_coinc_definer_table(connection)
    overlap_utils.create_coinc_event_table(connection)
    # create a table to store all of the overlaps;
    # we will delete this when done
    overlap_utils.create_all_results_table(connection)
    # ditto the template weight table
    overlap_utils.create_tmplt_weights_table(connection)

    
    # create the log table for checkpointing
    overlap_utils.create_log_table(connection)
    # get startup parameters
    startup_dict = overlap_utils.get_startup_data(connection)
    if startup_dict != {}:
        if opts.verbose:
            print >> sys.stdout, "Loaded startup data..."

        # get the backup archive if we're caching to disk
        backup_arxv_fn = startup_dict['backup_archive']
        if backup_arxv_fn is None:
            if opts.use_injection_cache == 'disk':
                archive = waveform_utils.get_scratch_archive(tmp_path,
                    start_archive=backup_arxv_fn)
            else:
                archive = {}
        elif opts.use_injection_cache == 'disk':
            # load the archive
            if backup_arxv_fn.endswith('.hdf5'):
                archive = waveform_utils.get_scratch_archive(tmp_path,
                    start_archive=backup_arxv_fn)
            else:
                # if the last program used a backup archive in memory,
                # we cannot use it; so we just start from scratch
                print >> sys.stderr, "Warning: Backup archive appears to " +\
                    "have been from a dictionary. Will recreate the archive "+\
                    "from scratch as an hdf5 file."
                archive = waveform_utils.get_scratch_archive(tmp_path,
                    start_archive=None)
        elif (opts.use_injection_cache == 'memory' and
                backup_arxv_fn.endswith('.pickle')):
            archive = overlap_utils.load_backup_archive_dict(backup_arxv_fn)
        else:
            archive = {}

        # get the starting index
        start_idx = startup_dict['tmplt_num']
        # we add one to the injection index because the checkpoint always
        # happens after the injection was analyzed
        start_inj_idx = startup_dict['inj_num'] + 1

    else:
        start_idx = 0
        start_inj_idx = 0
        if opts.use_injection_cache == 'disk':
            archive = waveform_utils.get_scratch_archive(tmp_path,
                start_archive=None)
        else:
            archive = {}

    # get injections that were already analyzed
    sqlquery = """
        SELECT
            map.event_id, results.sample_rate, results.segment_length,
            results.num_tries
        FROM
            overlap_results as results
        JOIN
            coinc_event_map AS map
        ON
            map.coinc_event_id == results.coinc_event_id
        WHERE
            map.table_name == "sim_inspiral"
        """
    already_analyzed = dict([ [sim_id, (sr, sl, nt)] for (sim_id, sr, sl, nt)
        in connection.cursor().execute(sqlquery)])

    # get the templates
    templates = waveform_utils.TemplateDict()
    templates.get_templates(connection, opts.approximant, wfmin,
        amp_order=opts.amp_order, phase_order=opts.phase_order,
        spin_order=opts.spin_order, taper=taper,
        calc_f_final=True, estimate_dur=True, verbose=opts.verbose,
        only_matching=False)
    templates.clear_sigmas()
    if opts.verbose:
        print >> sys.stdout, "Database contains %i templates" %(
            len(templates.keys()))

    # order the templates by duration, sample_rate
    templates.set_sort_key(['duration', 'f_final'])

    # check that the tmplt_id matches; we only do this if we will be continuing
    # with filtering
    if startup_dict and not startup_dict["finished_filtering"] and \
            startup_dict['tmplt_id'] is not None:
        check_tid = templates.as_list[start_idx].tmplt_id
        if startup_dict['tmplt_id'] != check_tid:
            raise ValueError("starting tmplt_id-1 (%s) " %(check_tid) +\
                "does not match last tmplt_id in log table (%s)" %(
                startup_dict['tmplt_id']))
        elif opts.verbose:
            print >> sys.stdout, "Starting with template %i..." % (start_idx+1)

    # get the injections
    injections = waveform_utils.InjectionDict()
    injections.get_injections(connection, wfmin, archive=archive,
        calc_f_final=True, estimate_dur=True,
        verbose=opts.verbose)
    injections.clear_sigmas()
    # if vary n, set sort key
    if opts.vary_n:
        injections.set_sort_key(['duration', 'f_final'])

    # check that the simulation_id matches
    if startup_dict and not startup_dict["finished_filtering"] and \
            startup_dict['simulation_id'] is not None:
        check_id = str(injections.as_list[start_inj_idx - 1].simulation_id)
        if startup_dict['simulation_id'] != check_id:
            raise ValueError("starting simulation_id (%s) " %(check_id) +\
                "does not match last simulation_id in log table (%s)" %(
                startup_dict['simulation_id']))
        elif opts.verbose:
            print >> sys.stdout, "Starting with injection %i..." % (
                start_inj_idx+1)

    #
    #   Find the best match via fitting factor
    #
    if opts.verbose:
        verb_string = "Template %s%s: Injection %s%s\r"
        wpad1 = ''.join([' ' for vv in range(len(str(len(templates))))])
        wpad2 = ''.join([' ' for vv in range(len(str(len(injections))))])
   
    # we will keep track of the the previous segment lengths used
    # as we cycle over the injections and templates to minimize
    # the number of times we generate the waveforms;
    # inj seg lengths is a dictionary keyed by simulation_ids so as not to
    # regenerate the injection every time we regenerate the template
    last_inj_seg_lengths = {}


    #
    #
    #       Cycle over the templates
    #
    #

    # if we had finished filtering on the last checkpoint, we'll just skip
    # this section by making the start_idx be the same as the length as the
    # templates list
    if startup_dict == {}:
        finished_filtering = False
    else:
        finished_filtering = startup_dict["finished_filtering"]
    if finished_filtering:
        start_idx = len(templates.as_list)

    for jj in range(start_idx, len(templates.as_list)):

        tmplt = templates.as_list[jj]
        h = htilde = None
        last_tmplt_seg_length = 0

        # speed-up: if just applying no_anti_aligned weight, and this is an
        # anti-aligned template, just skip now
        if weight_functions == ['no_anti_aligned'] and \
                overlap_utils.noAntiAligned(tmplt) == 0.:
            continue

        #
        #
        #       Cycle over the injections 
        #
        #
        for ii, inj in enumerate(injections.as_list):
            # skip if already done at start
            if jj == start_idx and ii < start_inj_idx:
                continue

            if opts.verbose:
                lbl1 = str(jj+1)
                lbl2 = str(ii+1)
                print >> sys.stdout, verb_string %(wpad1[:-len(lbl1)], lbl1,
                    wpad2[:-len(lbl2)], lbl2),
                sys.stdout.flush()

            # check if the template falls within this injection's param window
            if opts.match_window_db is not None:
                pwList = param_windows[inj.approximant]
                pwin = pwList[pwList.find(inj.mchirp)]
            # note: if we're not using any windowing, the following will if
            # statement will always evaluate to False (i.e., filter
            # everything); see declaration of pwin above for reason
            if not pwin.in_recovered(inj.mchirp, tmplt.mchirp):
                continue

            # set the segment start time; we'll always make this to be
            # the injection's end_time - 1.5 times its estimated duration
            segment_start = inj.detector_end_time(ifo) - 1.5*inj.duration

            #
            #   If varying N, figure out the sample rate and segment
            #   lengths to use, and generate the template and injection
            #   waveforms accordingly
            #
            if opts.vary_n:
                # get the segment length to use
                # we'll set the segment length to be twice the max duration
                # of this template and the injection
                seg_length = int(2**(numpy.ceil(numpy.log2(
                    max(tmplt.duration, inj.duration)))+1))

                # Although we will eventually 0-pad in the FD to get a common
                # sample rate, we need to know what this common rate is for
                # PSD generation.
                sample_rate = int(2**(numpy.ceil(numpy.log2(
                    max(tmplt.f_final, inj.f_final))+1)))

                # since the duration estimates are based on the 2PN
                # approximation, the duration may be too short; we'll just put
                # a floor at 2s
                if seg_length < 2:
                    seg_length = 2
               
                N = sample_rate * seg_length
                df = 1./seg_length

                # if the segment length has changed, we need to regenerate
                # FD waveforms. For TD waveforms, we just need to 0-pad and
                # FFT. Note that if the TD waveforms have not been generated
                # yet, they will be generated here for the first time.

                #   The template:
                if seg_length != last_tmplt_seg_length:
                    if lalsim.SimInspiralImplementedTDApproximants(
                            lalsim.GetApproximantFromString(
                            tmplt.approximant)):
                        # TD waveform, zero-pad (or generate then zero-pad, if
                        # this is the first instance)
                        if h is None:
                            h = tmplt.get_td_waveform(tmplt.min_sample_rate,
                                tmplt.min_seg_length, store=False,
                                reposition=True)
                        # Because we sorted the injections by increasing
                        # duration, we can simply update the last h used with
                        # the extra 0's needed; i.e., this only ever increases
                        # the size of h, it never decreases.
                        pad_idx = int(tmplt.get_wraparound_dur() / h.delta_t)
                        h = waveform_utils.zero_pad_at_index(h, pad_idx,
                            tmplt.min_sample_rate*seg_length)
                        htilde = filter.make_frequency_series(h)
                    else:
                        # FD waveform, just have to regenerate
                        htilde = tmplt.get_fd_waveform(tmplt.min_sample_rate,
                            seg_length, store=False)
                    htilde = htilde * dyn_range_fac
                    htilde = htilde.astype(cmplx_type)
                    # update
                    last_tmplt_seg_length = seg_length

                #   The injection:
                # If the segment length has changed, we'll delete the shorter
                # length FD waveform from the archive as we will not need it
                # again (recall that the templates are sorted in increasing
                # duration) to save space
                last_inj_seg_length = last_inj_seg_lengths.setdefault(
                    str(inj.simulation_id), 0)
                if seg_length != last_inj_seg_length:
                    inj.del_from_archive(sample_rate, last_inj_seg_length, ifo,
                        segment_start, 'FD', del_unsegmented=False)
                    last_inj_seg_lengths[inj.simulation_id] = seg_length
                # now get/create the injection
                htildeprime = inj.get_fd_waveform(inj.min_sample_rate,
                    seg_length, ifo, segment_start,
                    store=opts.use_injection_cache is not None,
                    store_td=opts.use_injection_cache is not None)
                htildeprime = htildeprime * dyn_range_fac
                htildeprime = htildeprime.astype(cmplx_type)


            #
            # If not varying n, just generate/retrieve the waveforms at the
            # desired sample rate
            #
            else: 
                if htilde is None:
                    htilde = tmplt.get_fd_waveform(sample_rate, seg_length,
                        store=False)
                    htilde = htilde * dyn_range_fac
                    htilde = htilde.astype(cmplx_type)

                htildeprime = inj.get_fd_waveform(sample_rate, seg_length,
                    ifo, segment_start,
                    store=opts.use_injection_cache is not None)
                htildeprime = htildeprime * dyn_range_fac
                htildeprime = htildeprime.astype(cmplx_type)

            #
            #   Get the PSD and needed workspace vectors
            #
            if opts.asd_file is not None:
                psd = workSpace.get_psd_from_file(wfmin, sample_rate,
                    seg_length, opts.asd_file,
                    dyn_range_exp=opts.dyn_range_exp)
            else:
                psd = workSpace.get_psd(wfmin, sample_rate, seg_length,
                    opts.psd_model, dyn_range_exp=opts.dyn_range_exp)
            psd = psd.astype(real_type)

            # get work vectors
            out_work_mem = workSpace.get_out_vec(N,
                pytypes.complex_same_precision_as(htilde))
            corr_work_mem = workSpace.get_corr_vec(N,
                pytypes.complex_same_precision_as(htilde))
            if opts.vary_n:
                work_v1 = workSpace.get_work_FS_vec('v1', N/2+1, seg_length,
                    pytypes.complex_same_precision_as(htilde))
                work_v2 = workSpace.get_work_FS_vec('v2', N/2+1, seg_length,
                    pytypes.complex_same_precision_as(htildeprime))
                work_psd = workSpace.get_work_FS_vec('psd', N/2+1, seg_length,
                    pytypes.real_same_precision_as(psd))
            else:
                work_v1 = work_v2 = work_psd = None

            # get the normalization of htilde, htildeprime if they aren't known
            if inj.sigma is None:
                inj.sigma = filter.sigma(htildeprime, psd[:len(htildeprime)],
                    ofmin)
            if tmplt.sigma is None:
                tmplt.sigma = filter.sigma(htilde, psd[:len(htilde)], ofmin)

            #
            #   Compute effectualness
            #
            cmplx_ts, _, norm, maxidx, offset, _ = \
                overlap_utils.filter_by_resampling(
                htilde, htildeprime, psd, ofmin, target_sample_rate,
                max_resample_length=2, h_norm=tmplt.sigma**2, out=out_work_mem,
                corr_out=corr_work_mem, zero_pad_to_common_Nyquist=opts.vary_n,
                work_v1=work_v1, work_v2=work_v2, work_psd=work_psd)

            # Note: because we padded the segment length to be twice the
            # length of htildeprime and htilde, we do not need to worry
            # about corruption

            effectualness = abs(cmplx_ts[maxidx]*norm)/inj.sigma

            time_offset = segment_start + maxidx*cmplx_ts.delta_t + offset - \
                inj.geocent_time

            # calculate weight for this template if it hasn't been done yet
            if tmplt.weights == {}:
                for weight_func in weight_functions:
                    weight = \
                        overlap_utils.weight_functions[weight_func](
                            tmplt, ofmin, psd_model=opts.psd_model,
                            asd_file=opts.asd_file, workspace=workSpace,
                            use_tmplt_sigma=True)
                    tmplt.weights[weight_func] = weight 
                    overlap_utils.add_tmplt_weight(connection,
                        tmplt.tmplt_id, weight_func, weight)


            # store
            thisResult = overlap_utils.OverlapResult(tmplt, inj)
            thisResult.segment_length = seg_length
            thisResult.sample_rate = sample_rate
            thisResult.overlap_f_min = ofmin 
            thisResult.waveform_f_min = wfmin
            thisResult.tmplt_approximant = opts.approximant 
            thisResult.effectualness = effectualness
            thisResult.time_offset = time_offset.gpsSeconds
            thisResult.time_offset_ns = time_offset.gpsNanoSeconds
            thisResult.write_all_results_to_database(connection)

            # check time and update checkpoints if needed
            now = time.time()
            if (now - last_time)/60. > opts.checkpoint_dt:
                if opts.verbose:
                    print >> sys.stdout, "\ncheckpointing..."
                    sys.stdout.flush()
                # Note: never backing up archive
                archive = overlap_utils.checkpoint(connection, opts.output_dir,
                    archive, now, str(inj.simulation_id), ii,
                    str(tmplt.tmplt_id), jj, finished_filtering,
                    getpass.getuser(), backup_archive=False)
                if working_filename is not None:
                    # to avoid copy errors, we need to move the database over
                    # then move it back
                    dbtables.put_connection_filename(outfile, working_filename,
                        verbose=False)
                    working_filename = dbtables.get_connection_filename(
                        outfile, tmp_path=opts.tmp_space, verbose=False)
                    connection = sqlite3.connect(working_filename)
                    dbtables.set_temp_store_directory(connection,
                        opts.tmp_space, verbose = False)
                last_time = now

    #
    #   Best Match Calculations
    #

    # I don't have a good way to checkpoint this, but it should be
    # fairly quick. So we'll just force a checkpoint here, and assume that
    # the rest of the program can finish before the next checkpoint.
    # note that we set finished_filtering to True here
    if not finished_filtering:
        finished_filtering = True
        now = time.time()
        if opts.verbose:
            print >> sys.stdout, "\ncheckpointing..."
            sys.stdout.flush()
        archive = overlap_utils.checkpoint(connection, opts.output_dir,
            archive, now, str(injections.as_list[-1].simulation_id),
            len(injections.as_list)-1, str(templates.as_list[-1].tmplt_id),
            len(templates.as_list)-1, finished_filtering, getpass.getuser(),
            backup_archive=False)
        if working_filename is not None:
            # to avoid copy errors, we need to move the database over
            # then move it back
            dbtables.put_connection_filename(outfile, working_filename,
                verbose=False)
            working_filename = dbtables.get_connection_filename(outfile,
                tmp_path = opts.tmp_space, verbose=False)
            connection = sqlite3.connect(working_filename)
            dbtables.set_temp_store_directory(connection, opts.tmp_space,
                verbose=False)
        last_time = now

    if opts.verbose:
        print >> sys.stdout, "\nFinding best match:"

    best_match_query = """
        SELECT
            a.simulation_id, a.tmplt_id, a.effectualness, w.weight,
            a.time_offset, a.time_offset_ns, a.segment_length, a.sample_rate
        FROM
            all_results AS a
        JOIN
            tmplt_weights AS w
        ON
            w.tmplt_id == a.tmplt_id
        WHERE
            a.simulation_id == ? AND
            w.weight_function == ?
        ORDER BY
            a.effectualness * w.weight
        DESC LIMIT 1"""

    for ii, inj in enumerate(injections.as_list):

        if opts.verbose:
            print >> sys.stdout, "Injection %i          \r" % ii,
            sys.stdout.flush()
    

        for weight_func in weight_functions:
            # get the best match for this injection
            result = connection.cursor().execute(best_match_query,
                (str(inj.simulation_id), weight_func)).fetchone()
            if result is None:
                raise ValueError("Injection %s " %(inj.simulation_id) +\
                    "was not filtered with any template. This happens when " +\
                    "the windowing used is too small. Either remove the " +\
                    "injection from the output database (%s) " %(outfile) +\
                    "and re-run (make sure --replace is not on) or use a " +\
                    "larger window and re-run the entire job (with either " +\
                    "--replace on, or with a different user-tag).")
            simid, tmpltid, eff, wt, dt, dt_ns, seg_length, sample_rate = \
                result

            bestMatch = overlap_utils.OverlapResult(templates[tmpltid],
                injections[simid])
            bestMatch.effectualness = eff
            bestMatch.weight = wt
            bestMatch.time_offset = dt
            bestMatch.time_offset_ns = dt_ns
            bestMatch.segment_length = seg_length
            bestMatch.sample_rate = sample_rate
            bestMatch.overlap_f_min = ofmin 
            bestMatch.waveform_f_min = wfmin
            bestMatch.tmplt_approximant = opts.approximant 
          
            overlap_utils.write_result_to_database(connection, bestMatch,
                weight_func, this_process)

    if opts.verbose:
        print >> sys.stdout, ""
        sys.stdout.flush()
    
    # move onto next file
    if opts.use_injection_cache == 'disk':
        waveform_utils.close_scratch_archive(archive)
    # move the working database to the outfile and delete backups
    overlap_utils.clean_backup_files(connection)
    connection.commit()
    if not opts.save_all_overlaps:
        if opts.verbose:
            print >> sys.stdout, "Dropping all_results table and vacuuming..."
        connection.cursor().execute('DROP TABLE all_results')
        connection.commit()
        connection.cursor().execute('VACUUM')
        connection.commit()
    connection.close()
    if opts.tmp_space is not None:
        dbtables.put_connection_filename(outfile, working_filename,
            verbose=opts.verbose)

if opts.verbose:
    print >> sys.stdout, "Finished!"

sys.exit(0)
