#!/usr/bin/env python

__prog__ = 'pycbc_calc_exval'
__author__ = 'Collin Capano <collin.capano@ligo.org>'
__description__ = \
"""Calculates SNR, and chisq values for a set of signals given a set of
injections with matching templates. This will be done for every map found
between injections in a sim_inspiral table and templates in a sngl_inspiral
table. Values are stored for the best match, determined by SNR and New SNR."""

import sqlite3
import numpy
import os, sys, shutil
import time
import random
import pickle
import operator
from optparse import OptionParser

import lal
import lalsimulation as lalsim

from glue.ligolw import utils
from glue.ligolw import table
from glue.ligolw import lsctables
from glue.ligolw import dbtables
from glue.ligolw.utils import process

from pylal import ligolw_sqlutils as sqlutils

from pycbc import types as pytypes
from pycbc import filter
from pycbc import noise
from pycbc import vetoes
from pycbc.overlaps import waveform_utils, overlap_utils


parser = OptionParser(description = __description__)
parser.add_option('-o', '--output-dir', default = '.', help = 'Directory to save overlap output data. Default is current.')
parser.add_option("-t", "--tmp-space", action = "store", type = "string", default = None, metavar = "PATH", help = "Location of local disk on which to do work. This is used to enhance performance in a networked environment.")
parser.add_option("-p", "--psd-model", help = "What PSD model to use for overlaps. Options are %s." %(', '.join(overlap_utils.get_psd_models())))
parser.add_option("-A", "--asd-file", help = "Load the PSD from a dat file containing an ASD. If both this and psd-model specified, this will take precedence.")
parser.add_option('-f', '--waveform-f-min', type = 'float', help = "Frequency at which to start the waveform generation (in Hz).")
parser.add_option('-F', '--overlap-f-min', type = 'float', help = "Frequency at which to start the overlap (in Hz). Note: This must be larger than waveform-f-min.")
parser.add_option('-r', '--sample-rate', type = 'int', help = "Required. Sample rate to use (in Hz). Must be a power of 2. If vary-n is turned on and the sample rate used to generate the waveforms and is less than this, the effectulaness, snr, and chisq will be found by zero-padding the frequency-domain such that the time series will have this sample rate. If the sample rate used in the waveform generation is greater than this, that sample rate will be used.")
parser.add_option('-l', '--segment-length', type = 'int', help = "Segment length to use (in s). Must be a power of 2. Required if vary-n not used.")
parser.add_option('-N', '--vary-n', action='store_true', default=False, help='If turned on, the sample rate and segment length used will be set to the smallest possible value between a given template and injection pair.')
parser.add_option('-n', '--n-realizations', type = 'int', help = "Number of noise realizations to generate to calculate the mean snr, chisq, and new-snr.")
parser.add_option('-i', '--ifo', help = "What ifo to inject into. If none specified, no response function will be applied to the injections.")
parser.add_option('-U', '--user-tag', help = "User tag to add to all output files.")
parser.add_option('-a', '--approximant', help = "Approximant to use for the templates.")
parser.add_option("", "--amp-order", type=int, default=0, help="Amplitude order to use for templates. Default is 0.")
parser.add_option("", "--phase-order", type=int, default=7, help="Phase order to use for templates. Default is 7 (3.5PN).")
parser.add_option("", "--spin-order", type=int, default=None, help="Spin order to use for templates.")
parser.add_option("", "--taper", help = "For TD approximants, set whether or not to taper the templates at the start and/or end. Options are: start, end, start_end. If not specified, no tapering will be done.")
parser.add_option('-b', '--n-chisq-bins', type = 'int', help = 'Number of chisq bins to use.')
parser.add_option('-M', '--use-waveform-cache', metavar = '"memory" or "disk"', help = "Use a cache to save waveforms. This will speed up computation time if you have the same injection mapped to multiple templates, or the same template mapped to multiple injections. Options are 'memory' or 'disk'. If 'memory', all waveforms will be stored in memory. If 'disk', waveforms will be stored to a temporary h5py archive. If a temp-space is specified, this temporary archive will be placed there; otherwise, it will be created in the current working directory.")
parser.add_option('-T', '--checkpoint-dt', type = 'float', default = numpy.inf, help = 'Number of minutes between checkpoints. Default is infinity, meaning no checkpointing.')
parser.add_option('-R', '--replace', action='store_true', default=False, help = 'If the output database already exists, overwrite it. Default action is not to overwrite.')
parser.add_option('-X', '--overwrite-results', action = 'store_true', default = False, help = 'If snr, chisq, and new snr already exist in the database, overwrite them. Otherwise, results with these values populated will be skipped. Note that turning this option on effectively turns off checkpointing.')
parser.add_option('-S', '--seed', type = 'int', default = int((time.time()*100)% 1e6), help = 'Set the seed to use for the gaussian noise generator. If none specified, will use the current time.')
parser.add_option('-v', '--verbose', action = 'store_true', help = 'Be verbose.')

opts, filenames = parser.parse_args()

# check options
ifo = opts.ifo
if opts.ifo is not None:
    ifo = ifo.upper()
    if len(ifo) != 2:
        raise ValueError("--ifo must be of format [site][number], e.g., 'H1'")
wfmin = opts.waveform_f_min
ofmin = opts.overlap_f_min
if ofmin < wfmin:
    raise ValueError, "--waveform-f-min must be less than overlap-f-min"

if opts.sample_rate is None:
    raise ValueError("must specify a sample-rate")
target_sample_rate = opts.sample_rate
if numpy.log2(target_sample_rate) % 1 != 0.:
    raise ValueError("--sample-rate must be a power of 2")

if not opts.vary_n:
    sample_rate = target_sample_rate
    if opts.segment_length is None:
        raise ValueError("if not --vary-n, must specify a segment-length")
    seg_length = opts.segment_length
    if seg_length is None or numpy.log2(seg_length) % 1 != 0.:
        raise ValueError("--segment-length must be a power of 2")
    N = sample_rate * seg_length
    df = 1./seg_length

if opts.asd_file is None and opts.psd_model is None:
    raise ValueError("must specify --asd-file or --psd-model")
if opts.asd_file is not None and not os.path.exists(opts.asd_file):
    raise ValueError("asd-file %s not found" % opts.asd_file)

if not opts.n_realizations:
    raise ValueError, "--n-realizations required"

taper = waveform_utils.get_taper_string(opts.taper)

# create a waveform archive if desired
if opts.use_waveform_cache == 'disk':
    archive = waveform_utils.get_scratch_archive(
        opts.tmp_space is not None and opts.tmp_space or '.')
else:
    archive = {}

# set the start time
last_time = time.time()

# initialize the workspace
work_space = overlap_utils.WorkSpace()

# get the psd model to use
if opts.asd_file is None:
    psd_model = opts.psd_model

for fnum, infile in enumerate(filenames):

    if opts.verbose:
        print >> sys.stdout, "Analyzing file %s" % infile
    if not os.path.exists(infile):
        raise ValueError, "the input file %s could not be found" % infile

    outfile = overlap_utils.get_exval_outfilename(opts.output_dir, opts.ifo,
        opts.user_tag, fnum)
    if outfile == infile:
        raise ValueError, "output file (%s) is the same " % outfile +\
            "as the input file (%s)" %(infile)

    if opts.tmp_space:
        if os.path.exists(outfile) and not opts.replace:
            working_filename = dbtables.get_connection_filename(outfile,
                tmp_path = opts.tmp_space, verbose = opts.verbose)
        else:
            working_filename = dbtables.get_connection_filename(infile,
                tmp_path = opts.tmp_space, verbose = opts.verbose)
        connection = sqlite3.connect(working_filename)
        dbtables.set_temp_store_directory(connection, opts.tmp_space,
            verbose=opts.verbose)
    else:
        connection = overlap_utils.get_outfile_connection(infile, outfile,
            replace=opts.replace, verbose=opts.verbose)

    # FIXME: dbtables apparently has a problem with overlap_results table
    try:
        xmldoc = dbtables.get_xml(connection)
        this_process = process.register_to_xmldoc(xmldoc, __prog__,
            opts.__dict__).process_id
    except:
        print >> sys.stderr, "Warning: not writing process info to database"
        this_process = sqlutils.get_next_id(connection, 'process',
            'process_id')

    # get the injections and matching templates
    if opts.verbose:
        print >> sys.stdout, "Getting injections and matching templates..."
    inj_tmplt_map = overlap_utils.get_injection_template_map(connection)
    injections = waveform_utils.InjectionDict()
    injections.get_injections(connection, wfmin, archive=archive,
        calc_f_final=opts.vary_n, estimate_dur=True,
        verbose=opts.verbose)
    templates = waveform_utils.TemplateDict()
    templates.get_templates(connection, opts.approximant, wfmin,
        amp_order=opts.amp_order, phase_order=opts.phase_order,
        spin_order=opts.spin_order, taper=taper,
        archive=archive, calc_f_final=True, estimate_dur=True,
        verbose=opts.verbose, only_matching=True)

    # if we're overwriting results, easiest thing to do is to just drop the
    # results table, since we'll next create one if it doesn't exist
    if opts.overwrite_results:
        connection.cursor().execute('DROP TABLE IF EXISTS overlap_results') 

    # create the overlap results table if it doesn't exist yet 
    overlap_utils.create_results_table(connection)

    # get results that have already been analyzed: these are results
    # where the snr, chisq and new_snr are filled in
    # this is done by first deleting any results in the overlaps_results
    # table that do not have these values filled in, then getting the list
    # of coinc_event_ids that remain. This way, if we are calculating
    # expectation values from the output of a pycbc_overlaps job, all the
    # results will be written over.
    sqlquery = """
        DELETE FROM
            overlap_results
        WHERE
            snr IS NULL AND
            chisq IS NULL AND
            new_snr IS NULL
        """
    connection.cursor().execute(sqlquery)

    already_analyzed = [ceid for (ceid,) in connection.cursor().execute(
        'SELECT coinc_event_id FROM overlap_results')]

    # remove maps from the inj_tmplt_map table that were already analyzed
    if opts.verbose:
        skip_num = len(already_analyzed)
        print >> sys.stdout, "Skipping %i injections" % skip_num 
    for ceid in already_analyzed:
        inj_tmplt_map.pop(ceid)

    #
    #   Cycle over the results, calculating the expectation values for each
    #
    for ii, (ceid, (simid, tmpltid)) in enumerate(inj_tmplt_map.items()):

        if opts.verbose:
            print >> sys.stdout, "Injection %i\r" % (ii+1+skip_num),
            sys.stdout.flush()
       
        inj = injections[simid]
        tmplt = templates[tmpltid]

        # get the sample rate and segment length to use
        if opts.vary_n:
            sample_rate = int(2**(numpy.ceil(numpy.log2(
                max(tmplt.get_f_final(), inj.get_f_final()))+1)))

            # get the segment length to use
            # we'll set the segment length to be four times the max duration
            # of this template and the injection
            seg_length = int(2**(numpy.ceil(numpy.log2(
                max(tmplt.get_duration(), inj.get_duration())))+2))
            # since the duration estimates are based on the 2PN
            # approximation, the duration may be too short; we'll just put
            # a floor at 2s
            if seg_length < 2:
                seg_length = 2

            df = 1./seg_length


        # set the segment start time; we'll always make this to be
        # the injection's end_time - 2.5 times its estimated duration
        segment_start = inj.detector_end_time(ifo) - 2.5*inj.get_duration()

        # get the FD injection and waveform
        # Note that these will also store the TD versions if they don't
        # exist in the archive
        htildeprime = inj.get_fd_waveform(sample_rate, seg_length,
            ifo, segment_start, store=opts.use_waveform_cache is not None)
        htilde = tmplt.get_fd_waveform(sample_rate, seg_length,
            store=opts.use_waveform_cache is not None)

        # get psd and needed workspace vectors
        if opts.asd_file is not None:
            psd = work_space.get_psd_from_file(df, wfmin, sample_rate,
                opts.asd_file)
        else:
            psd = work_space.get_psd(df, wfmin, sample_rate, psd_model)

        # we'll need a place to store work space vectors;
        # note that the length of these are set to the
        # target_sample_rate * the segment length
        N = target_sample_rate * seg_length
        kmax = N/2+1
        snr_work_mem = pytypes.zeros(N,
            dtype=pytypes.complex_same_precision_as(htilde))
        corr_work_mem = pytypes.zeros(N,
            dtype=pytypes.complex_same_precision_as(htilde))
        #snr_work_mem = None
        #corr_work_mem = None
        work_htilde = pytypes.FrequencySeries(pytypes.zeros(kmax,
            dtype=pytypes.complex_same_precision_as(htilde)),
            delta_f=htilde.delta_f)
        work_stilde = pytypes.FrequencySeries(pytypes.zeros(kmax,
            dtype=pytypes.complex_same_precision_as(htildeprime)),
            delta_f=htildeprime.delta_f)
        work_psd = pytypes.FrequencySeries(pytypes.ones(kmax,
            dtype=pytypes.real_same_precision_as(psd)),
            delta_f=psd.delta_f)

        # get the chisq bins
        chisq_bins = vetoes.power_chisq_bins(htilde, opts.n_chisq_bins, psd,
            ofmin)

        # compute the effectualness. We do this using filter_by_padding. If
        # the sample_rate is equal to the target_sample_rate, than this
        # will essentially just call filter.matched_filter_core.
        cmplx_ts, corr, norm = overlap_utils.filter_by_padding(htilde,
            htildeprime, psd, ofmin, target_sample_rate, work_v1=work_htilde,
            work_v2=work_stilde, work_psd=work_psd, out=snr_work_mem,
            corr_out=corr_work_mem)
        # normalize and drop the first and last quarter of the time series;
        # we don't really need to drop the first and last quarter for this,
        # it will just speed up calculations later on when computing the snr
        # in noise
        cmplx_ts = cmplx_ts[N/4:3*N/4]*norm
        sigmaprime = filter.sigma(htildeprime, psd, ofmin)

        maxidx = abs(cmplx_ts).data.argmax()
        effectualness = abs(cmplx_ts[maxidx])/sigmaprime
        offset = (maxidx + N/4) * cmplx_ts.delta_t

        time_offset = segment_start + offset - inj.geocent_time

        # cycle over the desired number of sample times, calculating snr,
        # chisq, and newsnr for each
        snrs = []
        chisq_vals = []
        newsnrs = []
        num_successes = 0
        for nn in range(opts.n_realizations):
            # create an instance of the noise
            # pycbc's noise package will give the same noise if you pass the
            # seed argument everytime. We therefore only pass it a seed on
            # the first pass.
            if nn == 0:
                seed = opts.seed
            else:
                seed = None
            ntilde = noise.frequency_noise_from_psd(psd, seed=seed)

            # check what the loudest event in noise is
            snr_work_mem = pytypes.zeros(N,
                dtype=pytypes.complex_same_precision_as(htilde))
            corr_work_mem = pytypes.zeros(N,
                dtype=pytypes.complex_same_precision_as(htilde))
            rand_ts, rand_corr, rand_norm = overlap_utils.filter_by_padding(
                htilde, ntilde, psd, ofmin, target_sample_rate,
                work_v1=work_htilde, work_v2=work_stilde, work_psd=work_psd, 
                out=snr_work_mem, corr_out=corr_work_mem)

            # Note: norm and rand_norm should be the same, so we'll just use
            # norm for the normalization
            rand_ts = rand_ts[N/4:3*N/4] * norm
            randsnr = abs(rand_ts).max()


            # the total snr is the sum of the complex time series from the
            # noise and from the injection
            rand_ts._epoch = cmplx_ts._epoch
            snr_ts = cmplx_ts + rand_ts
            maxidx = abs(snr_ts).data.argmax()
            snr = abs(snr_ts[maxidx])

            # only calculate chisq if the snr is louder than noise
            if snr > randsnr:
                # the corr to use is just the rand_corr + the injection corr
                chisq_corr = rand_corr + corr
                # Note: the indx of the maximum has to be with respect to
                # the full time series, not to the cropped time series
                chisq_indx = pytypes.Array(numpy.array([maxidx+N/4]))
                chisq = vetoes.power_chisq_at_points_from_precomputed(
                    chisq_corr, pytypes.Array(numpy.array([snr/norm])),
                    norm, chisq_bins, chisq_indx)[0]

                newsnr = overlap_utils.new_snr(snr, chisq,
                    2*opts.n_chisq_bins-2)

                snrs.append(snr)
                chisq_vals.append(chisq)
                newsnrs.append(newsnr)
                num_successes += 1

        # store the results
        this_result = overlap_utils.OverlapResult(tmplt, inj)
        this_result.segment_length = seg_length
        this_result.sample_rate = int(1./cmplx_ts.delta_t)
        this_result.overlap_f_min = ofmin 
        this_result.waveform_f_min = wfmin
        this_result.tmplt_approximant = opts.approximant 
        this_result.effectualness = effectualness
        this_result.time_offset = time_offset.gpsSeconds
        this_result.time_offset_ns = time_offset.gpsNanoSeconds

        # compute mean, st. devs.
        if num_successes > 0:
            snrs = numpy.array(snrs)
            chisq_vals = numpy.array(chisq_vals)
            newsnrs = numpy.array(newsnrs)
            this_result.snr = snrs.mean()
            this_result.snr_std = snrs.std()
            this_result.chisq = chisq_vals.mean()
            this_result.chisq_std = chisq_vals.std()
            this_result.chisq_dof = 2*opts.n_chisq_bins - 2
            this_result.new_snr = newsnrs.mean()
            this_result.new_snr_std = newsnrs.std()
        else:
            this_result.snr = 0
            this_result.snr_std = 0
            this_result.chisq = 0
            this_result.chisq_std = 0
            this_result.chisq_dof = 0
            this_result.new_snr = 0
            this_result.new_snr_std = 0

        this_result.num_tries = opts.n_realizations
        this_result.num_successes = num_successes

        # dump to output database
        this_result.write_to_database(connection, ceid)

        connection.commit()

        # check time and update checkpoints if needed
        now = time.time()
        if (now - last_time)/60. > opts.checkpoint_dt and \
                opts.tmp_space is not None:
            if opts.verbose:
                print "\ncheckpointing..."
            overlap_utils.copy_to_output(working_filename,
                outfile, verbose=False)
            last_time = now

    if opts.verbose:
        print >> sys.stdout, ""
        sys.stdout.flush()
    
    # close and exit 
    connection.commit()
    connection.close()
    if opts.tmp_space is not None:
        dbtables.put_connection_filename(outfile, working_filename,
            verbose=opts.verbose)

if opts.verbose:
    print >> sys.stdout, "Finished!"

sys.exit(0)
